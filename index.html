
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="workshop, computer vision, audio processing, computer graphics, visual learning, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon.png">

  <title>M3DS @CVPR24</title>
  <meta name="description" content="Multimodalities for 3D Scenes, CVPR 2024 Workshop">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Multimodalities for 3D Scenes Workshop"/>
  <meta property="og:url" content="https://multimodalitiesfor3dscenes.github.io"/>
  <meta property="og:description" content="Multimodalities for 3D Scenes, CVPR 2022 Workshop"/>
  <meta property="og:site_name" content="Multimodalities for 3D Scenes Workshop"/>
  <meta property="og:image" content="https://multimodalitiesfor3dscenes.github.io/static/img/site/teaser.jpg"/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Multimodalities for 3D Scenes Workshop"/>
  <meta name="twitter:image" content="https://multimodalitiesfor3dscenes.github.io/static/img/site/teaser.jpg">
  <meta name="twitter:url" content="https://multimodalitiesfor3dscenes.github.io"/>
  <meta name="twitter:description" content="Multimodalities for 3D Scenes, CVPR 2024 Workshop"/>

  <!-- CSS  -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 125px;
      max-height: 125px;
      /*width:300px;*/
      /*height:300px;*/
      object-fit: cover;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Call for papers</a></li>
        <li><a href="#dates">Schedule</a></li>
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
        <!-- <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../CVPR2021/index.html" target="__blank">CVPR 2021</a></li>
            <li><a href="../ECCV2022/" target="__blank">ECCV 2022</a></li>
          </ul>
        </li> -->
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>1st Workshop on Multimodalities for 3D Scenes</h1></center>
    <center><h2>CVPR 2024 Workshop</h2></center>
    <!-- <center>Room W03 - October 3 (2:00 - 5:50 pm), 2023</center> -->
  </div>
</div>

<hr />


<!-- <br>
  <center>
  <h1 style="color:red"><a href="https://www.youtube.com/watch?v=gyJDGrbLknI">The <b>video recording</b> of this workshop is here!</a></h1>
  </center>
<br> -->

<!-- <div class="alert alert-info" role="alert">
  <b>Join Zoom Meeting  <a href="https://kaust.zoom.us/j/95818223470">here</a>.</b>
</div> -->



<!-- <div class="row" id="teaser">  
    <div>  
    <img src="static/img/site/teaser.jpg" style="width: 100%; height: auto;"/>
  </div>
</div> -->


<!-- <div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/a.png">
</div>

<div class="col-xs-6 col-sm-6 col-md-6 col-lg-6"> 
  <img src="static/img/site/b.png">
</div>
 -->






<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Human sensory experiences such as vision, audio, touch, and smell are the natural interfaces to perceive the world around us 
      and reason about our environments. Understanding the 3D environments around us is important for many applications such as 
      video processing, robotics, or augmented reality. While there have been a lot of efforts in understanding 3D scenes in recent 
      years, most works (workshops) focus on mainly using vision to understand 3D scenes. However, vision alone does not fully 
      capture the properties of 3D scenes, e.g., the materials of objects and surfaces, the affordance of objects, and the acoustic 
      properties. In addition, humans use language to describe 3D scenes, and understanding 3D scenes from languages is also of 
      vital importance. 

      We believe the future is to model and understand 3D scenes and objects with rich multi-sensory inputs, including but not 
      limited to vision, language, audio, and touch. The goal of this workshop is to unite researchers from these different 
      sub-communities and move towards scene understanding with multi-modalities. We want to share the recent progress of 
      multimodal scene understanding, and also to discuss which directions the field should investigate next.
      </p>
  </div>
</div>

<p><br /></p>

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call For Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
      <p>
        <span style="font-weight:500;">Call for papers:</span> We invite non-archival papers of up to 8 pages (in CVPR format) for work 
        on tasks related to the intersection of multimodalities and 3D object understanding in real-world scenes.
        Paper topics may include but are not limited to:
      </p>
      <ul>
        <li>3D Visual Grounding</li>
        <li>3D Dense Captioning</li>
        <li>3D Question Answering</li>
        <li>Audio-visual 3D scene reconstruction and mapping</li>
        <li>Modeling scene acoustics from visuals</li>
        <li>Material prediction with visual/audio/tactile inputs</li>
        <li>Implicit multimodal neural field of 3D scenes and objects</li>
        <li>Multimodal simulation of 3D scenes and objects </li>
      </ul>
      <p>
        <span style="font-weight:500;">Submission:</span> We encourage submissions of up to 8 pages, excluding references and acknowledgements.
        The submission should be in the CVPR format.
        Reviewing will be single-blind.
        Accepted papers will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
        We welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
        Please submit your paper to the following address by the deadline: 
        <span style="color:#1a1aff;font-weight:400;"><a href="mailto:multimodalities3dscenes@gmail.com">multimodalities3dscenes@gmail.com</a></span>
        Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
      </p>
  </div>
</div>                                                                                        

<!-- <p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <table>
      <tbody> 
      <tr><td><a href="https://arxiv.org/abs/2212.01558">#1. PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, Hao Su</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2112.08359">#2. 3D Question Answering</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.11682">#3. PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, Peng Gao</font></td> </tr>        
      <tr><td><a href="https://arxiv.org/abs/2211.16312">#4. PLA: Language-Driven Open-Vocabulary 3D Scene Understanding</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, Xiaojuan Qi</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2303.16894">#5. ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2211.15654">#6.  OpenScene: 3D Scene Understanding with Open Vocabularies</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Songyou Peng, Kyle Genova, Chiyu "Max" Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser</font></td></tr>
      <tr><td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kurita_RefEgo_Referring_Expression_Comprehension_Dataset_from_First-Person_Perception_of_Ego4D_ICCV_2023_paper.pdf">#7. RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D </a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shuhei Kurita, Naoki Katsura, Eri Onami</font></td></tr>
      <tr><td><a href="https://arxiv.org/abs/2303.12236">#8. SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung</font></td></tr>
      <tr><td><a href="https://3d-vista.github.io">#9. 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li</font></td></tr>
    </tbody></table>
  </div>

</div> -->

<p><br /></p>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper submission deadline</td>
          <td>April 15, 2024</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td>April 22, 2024</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>April 29, 2024</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td>June 17, 2024 </td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome</td>
          <td>2:00pm - 2:05pm</td>
        </tr>
        <tr>
          <td>Invited Talk</td>
          <td>2:05pm - 2:30pm</td>
        </tr>
        <tr>
          <td>Invited Talk</td>
          <td>2:30pm - 2:55pm</td>
        </tr>
        <tr>
          <td>Poster session / Coffee break</td>
          <td>3:00pm - 3:25pm</td>
        </tr>
        <tr>
          <td>Invited Talk</td>
          <td>3:30pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Talk</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Paper spotlights</td>
          <td>4:30pm - 4:55pm</td>
        </tr>
        <tr>
          <td>Invited Talk</td>
          <td>5:00pm - 5:40pm</td>
        </tr>
        <tr>
          <td>Concluding Remarks</td>
          <td>5:40pm - 5:50pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<p><br /></p>
<div class="row">
  <div class="col-md-2">
    <a href="https://xiaolonw.github.io/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/xiaolong.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://xiaolonw.github.io/">Xiaolong Wang</a></b> is an Assistant Professor of the ECE department at the University of California, San Diego. He is affiliated with the CSE department, Center for Visual Computing, Contextual Robotics Institute, Artificial Intelligence Group, and the TILOS NSF AI Institute. He received his Ph.D. in Robotics at Carnegie Mellon University. His postdoctoral training was at the University of California, Berkeley. His research focuses on the intersection between computer vision and robotics. He is particularly interested in learning visual representation from videos in a self-supervised manner and uses this representation to guide robots to learn. Xiaolong is the Area Chair of CVPR, AAAI, ICCV.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://andrewowens.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/owens.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://andrewowens.com/">Andrew Owens</a></b> is an assistant professor at The University of Michigan in the department of Electrical Engineering and Computer Science. Prior to that, he was a postdoctoral scholar at UC Berkeley. He received a Ph.D. in Electrical Engineering and Computer Science from MIT in 2016. He is a recipient of a Computer Vision and Pattern Recognition (CVPR) Best Paper Honorable Mention Award.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.cmu.edu/~katef/"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/katerina.png" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b> is an Assistant Professor in the Machine Learning Department at Carnegie Mellon.
      Prior to joining MLD's faculty she worked as a postdoctoral researcher first at UC Berkeley working with Jitendra Malik and then
      at Google Research in Mountain View working with the video group. Katerina is interested in building machines that understand the
      stories that videos portray, and, inversely, in using videos to teach machines about the world. The penultimate goal is
      to build a machine that understands movie plots, and the ultimate goal is to build a machine that would want to watch Bergman over this.
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-2">
    <a href="https://psych.indiana.edu/directory/faculty/smith-linda.html"><img class="people-pic" style="float:left;margin-right:50px;" src="static/img/people/smith.jpeg" /></a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://psych.indiana.edu/directory/faculty/smith-linda.html">Linda Smith</a></b> is a cognitive scientist recognized for her work on early object name learning as a form of statistical learning. Smith co-discovered infants' few-shot learning of object names, showed that few-shot learning is itself learned, and documented the relevant experiences. Smith was born in and grew up in Portsmouth, New Hampshire. She graduated from the University of Wisconsin (Madison) with a Bachelor of Science degree in experimental psychology and from the University of Pennsylvania with a Ph.D. in experimental psychology. She joined the faculty of Indiana University (Bloomington) in 1977 and is in the Department of Psychological and Brain Sciences and the Program in Cognitive Science. She won the David E. Rumelhart Prize for theoretical contributions to cognitive science and is a member of both the National Academy of Sciences and the American Academy of Arts and Science.</p>
  </div>
</div>
<p><br /></p>

<p><br /></p>

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://changan.io">
      <img class="people-pic" src="static/img/people/changan.png" />
    </a>
    <div class="people-name">
      <a href="https://changan.io">Changan Chen</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="static/img/people/angel.jpg" />
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://krrish94.github.io/">
      <img class="people-pic" src="static/img/people/krishna.png" />
    </a>
    <div class="people-name">
      <a href="https://krrish94.github.io/">Krishna Murthy</a>
      <h6>MIT</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://alexanderrichard.github.io/">
      <img class="people-pic" src="static/img/people/alex.jpg" />
    </a>
    <div class="people-name">
      <a href="https://alexanderrichard.github.io/">Alexander Richard</a>
      <h6>Meta Reality Labs Research</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~grauman/">
      <img class="people-pic" src="static/img/people/kristen.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~grauman/">Kristen Grauman</a>
      <h6>UT Austin, FAIR</h6>
    </div>
  </div>

</div>
<p><br /></p>
<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers please use <b>language3dscenes@gmail.com</b>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
